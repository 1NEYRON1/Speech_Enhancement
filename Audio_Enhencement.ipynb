{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6162de-f5c1-4ca9-a105-09b2b76ffaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from pesq import pesq\n",
    "from pystoi import stoi\n",
    "import soundfile as sf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11ceed6-7b8d-4b0d-aa45-bac1f2ee3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(777)\n",
    "np.random.seed(777)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed19ca86-4bfa-4e5b-9864-5f5f052a116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    sample_rate = 16000\n",
    "    n_fft = 512\n",
    "    hop_length = 128\n",
    "    win_length = 512\n",
    "\n",
    "    hidden_dim = 256\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "\n",
    "    batch_size = 4\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-3\n",
    "    gradient_clip = 5.0\n",
    "\n",
    "    train_clean_path = \"./data/wav_clean_train\"\n",
    "    train_noisy_path = \"./data/wav_noisy_train\"\n",
    "    test_clean_path = \"./data/wav_clean_test\"\n",
    "    test_noisy_path = \"./data/wav_noisy_test\"\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    results_path = \"./results\"\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9859cf47-71d4-4536-b9ff-29210264c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceBankDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, max_len=4*16000, mode='train'):\n",
    "        self.clean_dir = Path(clean_dir)\n",
    "        self.noisy_dir = Path(noisy_dir)\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.clean_files = sorted(list(self.clean_dir.glob(\"*.wav\")))\n",
    "        self.noisy_files = sorted(list(self.noisy_dir.glob(\"*.wav\")))\n",
    "        \n",
    "        if mode == 'test':\n",
    "            self.file_pairs = []\n",
    "            clean_dict = {f.name: f for f in self.clean_files}\n",
    "            for noisy_file in self.noisy_files:\n",
    "                clean_file = clean_dict.get(noisy_file.name)\n",
    "                if clean_file:\n",
    "                    self.file_pairs.append((clean_file, noisy_file))\n",
    "        else:\n",
    "            assert len(self.clean_files) == len(self.noisy_files), \\\n",
    "                f\"Mismatch: {len(self.clean_files)} clean vs {len(self.noisy_files)} noisy\"\n",
    "            self.file_pairs = list(zip(self.clean_files, self.noisy_files))\n",
    "        \n",
    "        print(f\"Loaded {len(self.file_pairs)} file pairs for {mode} mode\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        clean_file, noisy_file = self.file_pairs[idx]\n",
    "        \n",
    "        clean_wav, sr = torchaudio.load(clean_file)\n",
    "        noisy_wav, _ = torchaudio.load(noisy_file)\n",
    "        \n",
    "        if clean_wav.shape[0] > 1:\n",
    "            clean_wav = clean_wav.mean(dim=0, keepdim=True)\n",
    "        if noisy_wav.shape[0] > 1:\n",
    "            noisy_wav = noisy_wav.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if self.mode == 'train' and self.max_len:\n",
    "            if clean_wav.shape[1] > self.max_len:\n",
    "                start = torch.randint(0, clean_wav.shape[1] - self.max_len, (1,))\n",
    "                clean_wav = clean_wav[:, start:start+self.max_len]\n",
    "                noisy_wav = noisy_wav[:, start:start+self.max_len]\n",
    "            else:\n",
    "                pad_len = self.max_len - clean_wav.shape[1]\n",
    "                clean_wav = F.pad(clean_wav, (0, pad_len))\n",
    "                noisy_wav = F.pad(noisy_wav, (0, pad_len))\n",
    "        \n",
    "        return {\n",
    "            'noisy': noisy_wav.squeeze(0),\n",
    "            'clean': clean_wav.squeeze(0),\n",
    "            'filename': clean_file.name,\n",
    "            'clean_path': str(clean_file),\n",
    "            'noisy_path': str(noisy_file)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4527afb-22ee-4d42-8dc7-1435db67e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFT(nn.Module):\n",
    "    def __init__(self, n_fft=512, hop_length=128, win_length=512):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = torch.hann_window(win_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.window = self.window.to(x.device)\n",
    "        spec = torch.stft(\n",
    "            x,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "            return_complex=True,\n",
    "        )\n",
    "        return spec\n",
    "\n",
    "    def inverse(self, spec):\n",
    "        self.window = self.window.to(spec.device)\n",
    "        wav = torch.istft(\n",
    "            spec,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "        )\n",
    "        return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82fb7bc4-1e22-487f-b2d4-16c98d5712ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xLSTMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Упрощенная реализация xLSTM блока для бейзлайна\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(hidden_dim * 2, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, n_freq, C = x.shape\n",
    "\n",
    "        # Применяем LSTM по временной оси\n",
    "        x_reshaped = x.permute(0, 2, 1, 3).reshape(B * n_freq, T, C)\n",
    "        out, _ = self.lstm(x_reshaped)\n",
    "        out = self.norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.proj(out)\n",
    "        out = out.reshape(B, n_freq, T, C).permute(0, 2, 1, 3)\n",
    "\n",
    "        return out + x  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fc4d09a-c011-4d55-9872-b95d96981b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Упрощенная реализация Mamba блока для бейзлайна\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Используем Conv1D как аппроксимацию SSM\n",
    "        self.conv = nn.Conv1d(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            groups=input_dim // 4 if input_dim >= 4 else 1,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(hidden_dim, input_dim)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, n_freq, C = x.shape\n",
    "\n",
    "        # Применяем по частотной оси\n",
    "        x_reshaped = x.permute(0, 1, 3, 2).reshape(B * T, C, n_freq)\n",
    "        out = self.conv(x_reshaped)\n",
    "        out = out.permute(0, 2, 1).reshape(B, T, n_freq, -1)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out + x  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a4648-7016-4971-aa44-95abee45a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Комбинация xLSTM (для времени) и Mamba (для частоты)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.xlstm = xLSTMBlock(input_dim, hidden_dim, dropout)\n",
    "        self.mamba = MambaBlock(input_dim, hidden_dim, dropout)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.xlstm(x)\n",
    "        x = self.mamba(x)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff35825c-e799-45af-8066-496468fb5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            HybridBlock(hidden_dim, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13fcdb6d-5be3-47ed-9966-33cf8caa8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnitudeDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [HybridBlock(hidden_dim, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.output_proj = nn.Sequential(nn.Linear(hidden_dim, output_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c6f40f7-d1ab-4659-a442-c8100568e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [HybridBlock(hidden_dim, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.output_proj = nn.Sequential(nn.Linear(hidden_dim, output_dim), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x * np.pi  # Масштабируем в [-pi, pi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e31c8-7c6f-4b6b-82b6-901a195377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPSENet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.stft = STFT(\n",
    "            n_fft=config.n_fft,\n",
    "            hop_length=config.hop_length,\n",
    "            win_length=config.win_length,\n",
    "        )\n",
    "\n",
    "        freq_dim = config.n_fft // 2 + 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=2,  # magnitude + phase\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "        # Параллельные декодеры\n",
    "        self.magnitude_decoder = MagnitudeDecoder(\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            output_dim=1,\n",
    "            num_layers=config.num_layers // 2,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "        self.phase_decoder = PhaseDecoder(\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            output_dim=1,\n",
    "            num_layers=config.num_layers // 2,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, noisy_wav):\n",
    "        original_length = noisy_wav.shape[-1]\n",
    "        \n",
    "        # STFT\n",
    "        noisy_spec = self.stft(noisy_wav)\n",
    "        noisy_mag = torch.abs(noisy_spec)\n",
    "        noisy_phase = torch.angle(noisy_spec)\n",
    "        \n",
    "\n",
    "        encoder_input = torch.stack([noisy_mag, noisy_phase], dim=-1)\n",
    "        encoder_input = encoder_input.permute(0, 2, 1, 3)\n",
    "        encoded = self.encoder(encoder_input)\n",
    "\n",
    "        enhanced_mag = self.magnitude_decoder(encoded).squeeze(-1).permute(0, 2, 1)\n",
    "        enhanced_phase = self.phase_decoder(encoded).squeeze(-1).permute(0, 2, 1)\n",
    "\n",
    "        enhanced_spec = torch.polar(enhanced_mag, enhanced_phase)\n",
    "        \n",
    "        # iSTFT\n",
    "        enhanced_wav = self.stft.inverse(enhanced_spec)\n",
    "\n",
    "        if enhanced_wav.shape[-1] > original_length:\n",
    "            enhanced_wav = enhanced_wav[..., :original_length]\n",
    "        elif enhanced_wav.shape[-1] < original_length:\n",
    "            pad_len = original_length - enhanced_wav.shape[-1]\n",
    "            enhanced_wav = F.pad(enhanced_wav, (0, pad_len))\n",
    "        \n",
    "        return enhanced_wav, enhanced_mag, enhanced_phase, noisy_mag, noisy_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7581020e-7c22-4760-96d7-9e77471dc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.2, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # вес для time-domain loss\n",
    "        self.beta = beta  # вес для magnitude loss\n",
    "        self.gamma = gamma  # вес для phase loss\n",
    "\n",
    "    def forward(\n",
    "        self, pred_wav, clean_wav, pred_mag, clean_mag, pred_phase, clean_phase\n",
    "    ):\n",
    "        # Time domain loss\n",
    "        time_loss = F.l1_loss(pred_wav, clean_wav)\n",
    "\n",
    "        # Magnitude loss\n",
    "        mag_loss = F.mse_loss(pred_mag, clean_mag)\n",
    "\n",
    "        # Phase loss\n",
    "        phase_diff = torch.cos(pred_phase - clean_phase)\n",
    "        phase_loss = 1 - phase_diff.mean()\n",
    "\n",
    "        total_loss = (\n",
    "            self.alpha * time_loss + self.beta * mag_loss + self.gamma * phase_loss\n",
    "        )\n",
    "\n",
    "        return total_loss, {\n",
    "            \"time_loss\": time_loss.item(),\n",
    "            \"mag_loss\": mag_loss.item(),\n",
    "            \"phase_loss\": phase_loss.item(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880d49b-76ec-48b1-8bcd-d5b6ec7c4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URGENTMetrics:\n",
    "    \"\"\"\n",
    "    Метрики из URGENT Challenge 2026\n",
    "    Репозиторий: https://github.com/urgent-challenge/urgent2026_challenge_track1\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_pesq(clean_path, enhanced_path, sr=16000):\n",
    "        \"\"\"PESQ - Perceptual Evaluation of Speech Quality\"\"\"\n",
    "        try:\n",
    "            clean, _ = sf.read(clean_path)\n",
    "            enhanced, _ = sf.read(enhanced_path)\n",
    "\n",
    "            min_len = min(len(clean), len(enhanced))\n",
    "            clean = clean[:min_len]\n",
    "            enhanced = enhanced[:min_len]\n",
    "\n",
    "            score = pesq(sr, clean, enhanced, \"wb\")\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing PESQ: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_stoi(clean_path, enhanced_path, sr=16000):\n",
    "        \"\"\"STOI - Short-Time Objective Intelligibility\"\"\"\n",
    "        try:\n",
    "            clean, _ = sf.read(clean_path)\n",
    "            enhanced, _ = sf.read(enhanced_path)\n",
    "\n",
    "            min_len = min(len(clean), len(enhanced))\n",
    "            clean = clean[:min_len]\n",
    "            enhanced = enhanced[:min_len]\n",
    "\n",
    "            score = stoi(clean, enhanced, sr, extended=False)\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing STOI: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_estoi(clean_path, enhanced_path, sr=16000):\n",
    "        \"\"\"Extended STOI\"\"\"\n",
    "        try:\n",
    "            clean, _ = sf.read(clean_path)\n",
    "            enhanced, _ = sf.read(enhanced_path)\n",
    "\n",
    "            min_len = min(len(clean), len(enhanced))\n",
    "            clean = clean[:min_len]\n",
    "            enhanced = enhanced[:min_len]\n",
    "\n",
    "            score = stoi(clean, enhanced, sr, extended=True)\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing eSTOI: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_si_sdr(clean_path, enhanced_path):\n",
    "        \"\"\"SI-SDR - Scale-Invariant Signal-to-Distortion Ratio\"\"\"\n",
    "        try:\n",
    "            clean, _ = sf.read(clean_path)\n",
    "            enhanced, _ = sf.read(enhanced_path)\n",
    "\n",
    "            min_len = min(len(clean), len(enhanced))\n",
    "            clean = clean[:min_len]\n",
    "            enhanced = enhanced[:min_len]\n",
    "\n",
    "            alpha = np.dot(enhanced, clean) / np.dot(clean, clean)\n",
    "            s_target = alpha * clean\n",
    "            e_noise = enhanced - s_target\n",
    "\n",
    "            si_sdr = 10 * np.log10(np.sum(s_target**2) / np.sum(e_noise**2))\n",
    "            return si_sdr\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing SI-SDR: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_all_metrics(clean_path, enhanced_path, sr=16000):\n",
    "        \"\"\"Вычисляет все метрики URGENT Challenge\"\"\"\n",
    "        return {\n",
    "            \"PESQ\": URGENTMetrics.compute_pesq(clean_path, enhanced_path, sr),\n",
    "            \"STOI\": URGENTMetrics.compute_stoi(clean_path, enhanced_path, sr),\n",
    "            \"eSTOI\": URGENTMetrics.compute_estoi(clean_path, enhanced_path, sr),\n",
    "            \"SI-SDR\": URGENTMetrics.compute_si_sdr(clean_path, enhanced_path),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a14640b-95eb-42fc-bee8-dd62abaafbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_components = {'time_loss': 0, 'mag_loss': 0, 'phase_loss': 0}\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        noisy = batch['noisy'].to(device)\n",
    "        clean = batch['clean'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        enhanced_wav, pred_mag, pred_phase, _, _ = model(noisy)\n",
    "        \n",
    "        clean_spec = model.stft(clean)\n",
    "        clean_mag = torch.abs(clean_spec)\n",
    "        clean_phase = torch.angle(clean_spec)\n",
    "        \n",
    "        loss, loss_dict = criterion(\n",
    "            enhanced_wav, clean, \n",
    "            pred_mag, clean_mag, \n",
    "            pred_phase, clean_phase\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        for key in loss_components:\n",
    "            loss_components[key] += loss_dict[key]\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_components = {k: v / len(train_loader) for k, v in loss_components.items()}\n",
    "    \n",
    "    return avg_loss, avg_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4839c45e-557f-4e20-89f2-a42b294dd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device, save_audio=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'PESQ': [], 'STOI': [], 'eSTOI': [], 'SI-SDR': []}\n",
    "    \n",
    "    enhanced_dir = Path(config.results_path) / \"enhanced_audio\"\n",
    "    if save_audio:\n",
    "        enhanced_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for batch in pbar:\n",
    "            noisy = batch['noisy'].to(device)\n",
    "            clean = batch['clean'].to(device)\n",
    "            filename = batch['filename'][0]\n",
    "            clean_path = batch['clean_path'][0]\n",
    "            \n",
    "            enhanced_wav, pred_mag, pred_phase, _, _ = model(noisy)\n",
    "            \n",
    "            clean_spec = model.stft(clean)\n",
    "            clean_mag = torch.abs(clean_spec)\n",
    "            clean_phase = torch.angle(clean_spec)\n",
    "            \n",
    "            loss, _ = criterion(\n",
    "                enhanced_wav, clean,\n",
    "                pred_mag, clean_mag,\n",
    "                pred_phase, clean_phase\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Сохраняем enhanced audio\n",
    "            if save_audio:\n",
    "                enhanced_path = enhanced_dir / filename\n",
    "            else:\n",
    "                enhanced_path = \"/tmp/\" + filename\n",
    "            \n",
    "            torchaudio.save(\n",
    "                str(enhanced_path),\n",
    "                enhanced_wav[0].cpu().unsqueeze(0),\n",
    "                config.sample_rate\n",
    "            )\n",
    "\n",
    "            batch_metrics = URGENTMetrics.compute_all_metrics(\n",
    "                clean_path,\n",
    "                str(enhanced_path),\n",
    "                sr=config.sample_rate\n",
    "            )\n",
    "            \n",
    "            for key, value in batch_metrics.items():\n",
    "                metrics[key].append(value)\n",
    "            \n",
    "            # Удаляем временный файл\n",
    "            if not save_audio and Path(enhanced_path).exists():\n",
    "                Path(enhanced_path).unlink()\n",
    "            \n",
    "            pbar.set_postfix({'PESQ': f\"{np.mean(metrics['PESQ']):.3f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c95f035-78d3-413c-9f6a-03a3cb12ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    Path(config.checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(config.results_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = VoiceBankDataset(\n",
    "        config.train_clean_path,\n",
    "        config.train_noisy_path,\n",
    "        mode='train'\n",
    "    )\n",
    "    test_dataset = VoiceBankDataset(\n",
    "        config.test_clean_path,\n",
    "        config.test_noisy_path,\n",
    "        max_len=None,\n",
    "        mode='test'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitializing MP-SENet model...\")\n",
    "    model = MPSENet(config).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params / 1e6:.2f}M\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    criterion = CombinedLoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_pesq': [],\n",
    "        'val_stoi': [],\n",
    "        'val_estoi': [],\n",
    "        'val_sisdr': []\n",
    "    }\n",
    "    \n",
    "    best_pesq = 0.0\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        train_loss, train_components = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Time: {train_components['time_loss']:.4f}, \"\n",
    "              f\"Mag: {train_components['mag_loss']:.4f}, \"\n",
    "              f\"Phase: {train_components['phase_loss']:.4f}\")\n",
    "\n",
    "        val_loss, val_metrics = validate(model, test_loader, criterion, device, save_audio=False)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Metrics:\")\n",
    "        print(f\"PESQ:   {val_metrics['PESQ']:.4f}\")\n",
    "        print(f\"STOI:   {val_metrics['STOI']:.4f}\")\n",
    "        print(f\"eSTOI:  {val_metrics['eSTOI']:.4f}\")\n",
    "        print(f\"SI-SDR: {val_metrics['SI-SDR']:.2f} dB\")\n",
    "\n",
    "        scheduler.step(val_metrics['PESQ'])\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_pesq'].append(val_metrics['PESQ'])\n",
    "        history['val_stoi'].append(val_metrics['STOI'])\n",
    "        history['val_estoi'].append(val_metrics['eSTOI'])\n",
    "        history['val_sisdr'].append(val_metrics['SI-SDR'])\n",
    "\n",
    "        if val_metrics['PESQ'] > best_pesq:\n",
    "            best_pesq = val_metrics['PESQ']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'pesq': best_pesq,\n",
    "                'metrics': val_metrics,\n",
    "                'config': config\n",
    "            }, f\"{config.checkpoint_path}/best_model.pt\")\n",
    "            print(f\"Saved best model with PESQ: {best_pesq:.4f}\")\n",
    "            \n",
    "            if best_pesq >= 3.25:\n",
    "                print(f\"Target PESQ {trainer_ner} achieved!\")\n",
    "    \n",
    "    pd.DataFrame(history).to_csv(f\"{config.results_path}/training_history.csv\", index=False)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20f172dd-d835-4213-8ce9-5f47856dc4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MP-SENet with xLSTM+Mamba for Speech Enhancement\n",
      "======================================================================\n",
      "\n",
      "[1] Training model...\n",
      "Loading datasets...\n",
      "Loaded 11572 file pairs for train mode\n",
      "Loaded 824 file pairs for test mode\n",
      "\n",
      "Initializing MP-SENet model...\n",
      "Total parameters: 6.28M\n",
      "\n",
      "Starting training...\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:43<00:00,  2.03it/s, loss=0.0687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1051\n",
      "Time: 0.0155, Mag: 0.0636, Phase: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:11<00:00,  4.31it/s, PESQ=1.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0649\n",
      "Val Metrics:\n",
      "PESQ:   1.7912\n",
      "STOI:   0.8896\n",
      "eSTOI:  0.7329\n",
      "SI-SDR: 15.43 dB\n",
      "Saved best model with PESQ: 1.7912\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:42<00:00,  2.03it/s, loss=0.0471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0572\n",
      "Time: 0.0079, Mag: 0.0379, Phase: 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:09<00:00,  4.35it/s, PESQ=2.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0634\n",
      "Val Metrics:\n",
      "PESQ:   2.0001\n",
      "STOI:   0.9024\n",
      "eSTOI:  0.7687\n",
      "SI-SDR: 17.27 dB\n",
      "Saved best model with PESQ: 2.0001\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:43<00:00,  2.03it/s, loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0557\n",
      "Time: 0.0074, Mag: 0.0327, Phase: 0.4397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:06<00:00,  4.41it/s, PESQ=2.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0633\n",
      "Val Metrics:\n",
      "PESQ:   2.0430\n",
      "STOI:   0.9039\n",
      "eSTOI:  0.7690\n",
      "SI-SDR: 15.95 dB\n",
      "Saved best model with PESQ: 2.0430\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:42<00:00,  2.03it/s, loss=0.0674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0549\n",
      "Time: 0.0071, Mag: 0.0300, Phase: 0.4390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:08<00:00,  4.38it/s, PESQ=1.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0671\n",
      "Val Metrics:\n",
      "PESQ:   1.8998\n",
      "STOI:   0.8942\n",
      "eSTOI:  0.7405\n",
      "SI-SDR: 15.22 dB\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:43<00:00,  2.03it/s, loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0544\n",
      "Time: 0.0070, Mag: 0.0284, Phase: 0.4385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:08<00:00,  4.36it/s, PESQ=2.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0619\n",
      "Val Metrics:\n",
      "PESQ:   2.1756\n",
      "STOI:   0.9178\n",
      "eSTOI:  0.7964\n",
      "SI-SDR: 17.92 dB\n",
      "Saved best model with PESQ: 2.1756\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:42<00:00,  2.03it/s, loss=0.0667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0537\n",
      "Time: 0.0067, Mag: 0.0262, Phase: 0.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 824/824 [03:05<00:00,  4.44it/s, PESQ=2.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0620\n",
      "Val Metrics:\n",
      "PESQ:   2.2428\n",
      "STOI:   0.9202\n",
      "eSTOI:  0.7971\n",
      "SI-SDR: 17.37 dB\n",
      "Saved best model with PESQ: 2.2428\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 711/2893 [05:50<17:54,  2.03it/s, loss=0.0651]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[1] Training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m train_loss, train_components \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMag: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmag_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphase_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss, loss_dict \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     19\u001b[0m     enhanced_wav, clean, \n\u001b[1;32m     20\u001b[0m     pred_mag, clean_mag, \n\u001b[1;32m     21\u001b[0m     pred_phase, clean_phase\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:38\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:219\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    217\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[1;32m    218\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 219\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:38\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:102\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[0;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     98\u001b[0m             [torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    101\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, norm_type\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MP-SENet with xLSTM+Mamba for Speech Enhancement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n[1] Training model...\")\n",
    "model, history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6d5a613-d66b-4317-8fab-173aac5b10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms(noisy_wav, enhanced_wav, clean_wav, sr=16000, save_path=None):\n",
    "    \"\"\"\n",
    "    Визуализирует спектрограммы noisy, enhanced и clean\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "    wavs = [noisy_wav, enhanced_wav, clean_wav]\n",
    "    titles = [\"Noisy\", \"Enhanced\", \"Clean\"]\n",
    "\n",
    "    for i, (wav, title) in enumerate(zip(wavs, titles)):\n",
    "        if isinstance(wav, torch.Tensor):\n",
    "            wav = wav.cpu().numpy()\n",
    "\n",
    "        D = librosa.stft(wav, n_fft=512, hop_length=128, win_length=512)\n",
    "        D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "        img = librosa.display.specshow(\n",
    "            D_db,\n",
    "            sr=sr,\n",
    "            hop_length=128,\n",
    "            x_axis=\"time\",\n",
    "            y_axis=\"hz\",\n",
    "            ax=axes[i],\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        axes[i].set_title(f\"{title} Spectrogram\", fontsize=14)\n",
    "        axes[i].set_ylabel(\"Frequency (Hz)\")\n",
    "        fig.colorbar(img, ax=axes[i], format=\"%+2.0f dB\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0505cc6b-f022-4275-8fe7-3e431818160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model, test_loader, num_samples=5):\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    enhanced_dir = Path(config.results_path) / \"audio_samples\"\n",
    "    enhanced_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Evaluating test set with URGENT Challenge metrics...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            noisy = batch['noisy'].to(device)\n",
    "            clean = batch['clean'].to(device)\n",
    "            filename = batch['filename'][0]\n",
    "            clean_path = batch['clean_path'][0]\n",
    "            noisy_path = batch['noisy_path'][0]\n",
    "\n",
    "            enhanced_wav, _, _, _, _ = model(noisy)\n",
    "\n",
    "            enhanced_path = enhanced_dir / filename\n",
    "            torchaudio.save(\n",
    "                str(enhanced_path),\n",
    "                enhanced_wav[0].cpu().unsqueeze(0),\n",
    "                config.sample_rate\n",
    "            )\n",
    "\n",
    "            metrics = URGENTMetrics.compute_all_metrics(\n",
    "                clean_path,\n",
    "                str(enhanced_path),\n",
    "                sr=config.sample_rate\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                **metrics\n",
    "            })\n",
    "\n",
    "            if sample_count < num_samples:\n",
    "                import shutil\n",
    "                shutil.copy(noisy_path, enhanced_dir / f\"{Path(filename).stem}_noisy.wav\")\n",
    "                shutil.copy(clean_path, enhanced_dir / f\"{Path(filename).stem}_clean.wav\")\n",
    "\n",
    "                plot_spectrograms(\n",
    "                    noisy[0].cpu(),\n",
    "                    enhanced_wav[0].cpu(),\n",
    "                    clean[0].cpu(),\n",
    "                    sr=config.sample_rate,\n",
    "                    save_path=str(enhanced_dir / f\"{Path(filename).stem}_spectrogram.png\")\n",
    "                )\n",
    "                \n",
    "                sample_count += 1\n",
    " \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"{config.results_path}/urgent_test_results.csv\", index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"URGENT CHALLENGE METRICS - Test Set Results\")\n",
    "    print(\"Model: MP-SENet with xLSTM + Mamba (Official Implementations)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"PESQ:   {results_df['PESQ'].mean():.4f} ± {results_df['PESQ'].std():.4f}  (Target: ≥3.25)\")\n",
    "    print(f\"STOI:   {results_df['STOI'].mean():.4f} ± {results_df['STOI'].std():.4f}\")\n",
    "    print(f\"eSTOI:  {results_df['eSTOI'].mean():.4f} ± {results_df['eSTOI'].std():.4f}\")\n",
    "    print(f\"SI-SDR: {results_df['SI-SDR'].mean():.2f} ± {results_df['SI-SDR'].std():.2f} dB\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Files passing PESQ ≥ 3.25: {(results_df['PESQ'] >= 3.25).sum()}/{len(results_df)} \"\n",
    "          f\"({(results_df['PESQ'] >= 3.25).sum() / len(results_df) * 100:.1f}%)\")\n",
    "    print(f\"Best PESQ: {results_df['PESQ'].max():.4f}\")\n",
    "    print(f\"Worst PESQ: {results_df['PESQ'].min():.4f}\")\n",
    "    print(f\"Median PESQ: {results_df['PESQ'].median():.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6135371f-3406-46ea-bd4b-bd77776a0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_df):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('MP-SENet Training History (xLSTM + Mamba)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history_df['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history_df['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PESQ\n",
    "    axes[0, 1].plot(history_df['val_pesq'], label='PESQ', color='green', linewidth=2)\n",
    "    axes[0, 1].axhline(y=3.25, color='r', linestyle='--', label='Target (3.25)', linewidth=2)\n",
    "    axes[0, 1].fill_between(range(len(history_df)), 3.25, history_df['val_pesq'], \n",
    "                            where=(history_df['val_pesq'] >= 3.25), alpha=0.3, color='green')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('PESQ')\n",
    "    axes[0, 1].set_title('Validation PESQ')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # STOI\n",
    "    axes[0, 2].plot(history_df['val_stoi'], label='STOI', color='orange', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('STOI')\n",
    "    axes[0, 2].set_title('Validation STOI')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # eSTOI\n",
    "    axes[1, 0].plot(history_df['val_estoi'], label='eSTOI', color='purple', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('eSTOI')\n",
    "    axes[1, 0].set_title('Validation eSTOI')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SI-SDR\n",
    "    axes[1, 1].plot(history_df['val_sisdr'], label='SI-SDR', color='brown', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('SI-SDR (dB)')\n",
    "    axes[1, 1].set_title('Validation SI-SDR')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # All metrics (normalized)\n",
    "    pesq_norm = (history_df['val_pesq'] - 1) / 3.5\n",
    "    axes[1, 2].plot(pesq_norm, label='PESQ (norm)', linewidth=2)\n",
    "    axes[1, 2].plot(history_df['val_stoi'], label='STOI', linewidth=2)\n",
    "    axes[1, 2].plot(history_df['val_estoi'], label='eSTOI', linewidth=2)\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Score (normalized)')\n",
    "    axes[1, 2].set_title('All Metrics Comparison')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.results_path}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d86f749-2d9f-4e30-bb90-fbb34c3f0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(model):\n",
    "    import time\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL ARCHITECTURE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Architecture: MP-SENet with xLSTM + Mamba\")\n",
    "    print(f\"\\nTotal parameters: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "    print(f\"Model size (FP32): {total_params * 4 / 1024**2:.2f} MB\")\n",
    "    print(f\"Model size (FP16): {total_params * 2 / 1024**2:.2f} MB\")\n",
    "\n",
    "    model.eval()\n",
    "    test_input = torch.randn(1, config.sample_rate * 4).to(device)  # 4 seconds\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(test_input)\n",
    "\n",
    "    num_runs = 100\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(test_input)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    rtf = avg_time / 4.0  # Real-Time Factor for 4s audio\n",
    "    \n",
    "    print(\"\\nINFERENCE SPEED ANALYSIS\")\n",
    "    print(f\"Average inference time: {avg_time*1000:.2f} ms (for 4s audio)\")\n",
    "    print(f\"Throughput: {4/avg_time:.2f} seconds audio per second\")\n",
    "    print(f\"Real-Time Factor (RTF): {rtf:.4f}\")\n",
    "    print(f\"Can process in real-time: {'Yes' if rtf < 1.0 else 'No'}\")\n",
    "    \n",
    "    if rtf < 1.0:\n",
    "        print(f\"\\n  Processing speed: {1/rtf:.2f}x faster than real-time\")\n",
    "    else:\n",
    "        print(f\"\\n  Processing speed: {rtf:.2f}x slower than real-time\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c9935f0-86f4-4442-9e5d-1ec9bda54401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparative_analysis(results_df):\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle('URGENT Challenge Metrics Analysis (xLSTM + Mamba)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # PESQ distribution\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.hist(results_df['PESQ'], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "    ax1.axvline(results_df['PESQ'].mean(), color='r', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean: {results_df['PESQ'].mean():.4f}\")\n",
    "    ax1.axvline(3.25, color='b', linestyle='--', linewidth=2, label='Target: 3.25')\n",
    "    ax1.set_xlabel('PESQ Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('PESQ Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # STOI distribution\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.hist(results_df['STOI'], bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "    ax2.axvline(results_df['STOI'].mean(), color='r', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean: {results_df['STOI'].mean():.4f}\")\n",
    "    ax2.set_xlabel('STOI Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('STOI Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # eSTOI distribution\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.hist(results_df['eSTOI'], bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "    ax3.axvline(results_df['eSTOI'].mean(), color='r', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean: {results_df['eSTOI'].mean():.4f}\")\n",
    "    ax3.set_xlabel('eSTOI Score')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('eSTOI Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # SI-SDR distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.hist(results_df['SI-SDR'], bins=20, edgecolor='black', alpha=0.7, color='brown')\n",
    "    ax4.axvline(results_df['SI-SDR'].mean(), color='r', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean: {results_df['SI-SDR'].mean():.2f} dB\")\n",
    "    ax4.set_xlabel('SI-SDR (dB)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('SI-SDR Distribution')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Correlation: PESQ vs STOI\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    ax5.scatter(results_df['PESQ'], results_df['STOI'], alpha=0.6, c='blue')\n",
    "    ax5.set_xlabel('PESQ')\n",
    "    ax5.set_ylabel('STOI')\n",
    "    ax5.set_title('PESQ vs STOI Correlation')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Correlation: PESQ vs SI-SDR\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.scatter(results_df['PESQ'], results_df['SI-SDR'], alpha=0.6, c='red')\n",
    "    ax6.set_xlabel('PESQ')\n",
    "    ax6.set_ylabel('SI-SDR (dB)')\n",
    "    ax6.set_title('PESQ vs SI-SDR Correlation')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plots\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    box_data = [results_df['PESQ'], results_df['STOI'], \n",
    "                results_df['eSTOI'], results_df['SI-SDR']/10]  # Scale SI-SDR for visualization\n",
    "    bp = ax7.boxplot(box_data, labels=['PESQ', 'STOI', 'eSTOI', 'SI-SDR/10'],\n",
    "                     patch_artist=True, showmeans=True)\n",
    "    colors = ['green', 'orange', 'purple', 'brown']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    ax7.set_ylabel('Score')\n",
    "    ax7.set_title('Metrics Box Plot Comparison')\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "    ax7.axhline(y=3.25, color='r', linestyle='--', alpha=0.5, label='PESQ Target')\n",
    "    ax7.legend()\n",
    "    \n",
    "    plt.savefig(f\"{config.results_path}/metrics_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed statistics\n",
    "    print(\"\\nDETAILED STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(results_df[['PESQ', 'STOI', 'eSTOI', 'SI-SDR']].describe())\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053a3e9e-2f96-4a22-9c15-6cad757d3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MP-SENet with xLSTM+Mamba for Speech Enhancement\n",
      "======================================================================\n",
      "\n",
      "[1] Training model...\n",
      "Loading datasets...\n",
      "Loaded 11572 file pairs for train mode\n",
      "Loaded 824 file pairs for test mode\n",
      "\n",
      "Initializing MP-SENet model...\n",
      "Total parameters: 6.28M\n",
      "\n",
      "Starting training...\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/1\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2893/2893 [23:44<00:00,  2.03it/s, loss=0.1668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1605\n",
      "Time: 0.0232, Mag: 0.0720, Phase: 1.2979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/824 [00:00<?, ?it/s]/tmp/ipykernel_3100/1134724928.py:12: UserWarning: Using a target size (torch.Size([1, 27861])) that is different to the input size (torch.Size([1, 27776])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  time_loss = F.l1_loss(pred_wav, clean_wav)\n",
      "Validation:   0%|          | 0/824 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (27776) must match the size of tensor b (27861) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[1] Training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 69\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMag: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmag_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphase_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m val_loss, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_audio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, criterion, device, save_audio)\u001b[0m\n\u001b[1;32m     21\u001b[0m clean_mag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(clean_spec)\n\u001b[1;32m     22\u001b[0m clean_phase \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mangle(clean_spec)\n\u001b[0;32m---> 24\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43menhanced_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_mag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_mag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_phase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_phase\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Сохраняем enhanced audio\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mCombinedLoss.forward\u001b[0;34m(self, pred_wav, clean_wav, pred_mag, clean_mag, pred_phase, clean_phase)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m, pred_wav, clean_wav, pred_mag, clean_mag, pred_phase, clean_phase\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Time domain loss\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     time_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Magnitude loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     mag_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_mag, clean_mag)\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/nn/functional.py:3810\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3808\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3810\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m~/.mlspace/envs/gigatts/lib/python3.10/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (27776) must match the size of tensor b (27861) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MP-SENet with xLSTM+Mamba for Speech Enhancement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n[1] Training model...\")\n",
    "model, history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be4d5d-ec47-4c82-9a13-28b7031262d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация истории обучения\n",
    "print(\"\\n[2] Plotting training history...\")\n",
    "history_df = pd.read_csv(f\"{config.results_path}/training_history.csv\")\n",
    "plot_training_history(history_df)\n",
    "\n",
    "# Загружаем лучшую модель\n",
    "print(\"\\n[3] Loading best model...\")\n",
    "checkpoint = torch.load(f\"{config.checkpoint_path}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "# Анализ производительности\n",
    "print(\"\\n[4] Analyzing model performance...\")\n",
    "analyze_model_performance(model)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "print(\"\\n[5] Evaluating on test set...\")\n",
    "test_dataset = VoiceBankDataset(\n",
    "    config.test_clean_path, config.test_noisy_path, max_len=None\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "results_df = evaluate_test_set(model, test_loader, num_samples=5)\n",
    "\n",
    "# Сравнительный анализ\n",
    "print(\"\\n[6] Performing comparative analysis...\")\n",
    "comparative_analysis(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All tasks completed successfully!\")\n",
    "print(f\"Results saved to: {config.results_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944a6e0-ac37-47cc-9b84-9bc3af1d9141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79367bf8-6f66-4125-b5ce-3723fb2e06ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8b25a-0768-47d8-950e-d969a1e9d77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebaf2a-22da-43e1-a3b6-20c157d5db16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f37f9-8f65-4162-a446-6cc454763a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402508fd-6abc-4548-834c-2367409333fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-gigatts]",
   "language": "python",
   "name": "conda-env-.mlspace-gigatts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
